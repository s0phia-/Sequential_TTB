from collections import defaultdict
import numpy as np


class ExploitQ:
    def __init__(self, qq, num_features=4):
        self.qq = defaultdict(lambda: np.zeros(4))
        for k, v in qq.items():
            self.qq[k] = v
        self.validities = defaultdict(lambda: np.zeros(4))
        self.num_features = num_features

    def choose_action(self, state, after_states):
        state = tuple(state)  # make state hashable
        if sum(self.validities[state]) == 0:
            validities = self.calc_validities(state, after_states)
        else:
            validities = self.validities[state]
        _, after_state = self.ttb_action(validities, after_states)
        action = after_state[0:2] - state[0:2]
        return action

    def calc_validities(self, state, after_states):
        """
        Feature validity of feature i is calculated as max_a(Q(s,a)) for the after state s that would be chosen with
        feature i set as the best feature. There is no conditionality in this calculation
        :param state: current state
        :param after_states: states each of the actions would lead to
        :return: a list of feature validities, indexed in the same order as the state features
        """
        feature_validities = []
        for f in range(self.num_features):
            feature_values = [a[f] for a in after_states]  # value of feature f for each of the after states
            chosen_after_state = after_states[np.argmax(feature_values)]  # after state with top feature value
            feature_validity = max(self.qq[tuple(chosen_after_state)])
            feature_validities.append(feature_validity)
        self.validities[state] = feature_validities
        return feature_validities

    def ttb_action(self, feature_importance, after_states):
        """
        implements the ttb heuristic for action selection

        :param feature_importance: a ranking of the importance of features
        :param after_states: actions to choose from
        :return: the best action, according to TTB
        """

        original_actions = after_states
        after_states = np.unique(after_states, axis=0)
        feature_importance = np.array(feature_importance, dtype=float)
        for i in range(self.num_features):
            # TODO add in something for feature direction
            best_feature = np.nanargmax(feature_importance)
            feature_values = [a[best_feature] for a in after_states]
            if np.sum(feature_values == max(feature_values)) == 1:  # if there is 1 action with the max feature value
                action_ix = np.argmax(feature_values)  # select that action. Otherwise, keep looping through features
                break
            else:  # find the next best deciding feature - delete previous best. Only keep after_states which were best
                after_states = after_states[np.argwhere(feature_values == np.max(feature_values)).flatten()]
                feature_importance[best_feature] = np.nan
        else:
            action_ix = np.random.choice(after_states.shape[0])
        best_after_state = after_states[action_ix]
        whole_list_action_ix = int(np.where(np.all(original_actions == best_after_state, axis=1))[0][0])
        return whole_list_action_ix, best_after_state
