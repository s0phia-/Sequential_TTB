from collections import defaultdict
import numpy as np


class ExploitQ:
    def __init__(self, num_features=8, *args):  # qq
        # self.qq = defaultdict(lambda: np.zeros(4))
        # for k, v in qq.items():
        #     self.qq[k] = v
        self.validities = defaultdict(lambda: np.zeros(4))
        self.num_features = num_features

    def qq(self, x):
        return -4 * x[2] - x[4] - x[5] - x[1] - x[3] + x[6]

    def choose_action(self, state, after_states):
        state = tuple(state)  # make state hashable
        if sum(self.validities[state]) == 0:
            validities = self.calc_validities(state, after_states)
        else:
            validities = self.validities[state]
        action_ix, after_state = self.ttb_action(validities, after_states)
        return action_ix

    def calc_validities(self, state, after_states):
        """
        Feature validity of feature i is calculated as max_a(Q(s,a)) for the after state s that would be chosen with
        feature i set as the best feature. There is no conditionality in this calculation
        :param state: current state
        :param after_states: states each of the actions would lead to
        :return: a list of feature validities, indexed in the same order as the state features
        """

        feature_validities = []
        for f in range(self.num_features):
            feature_values = [a[f] for a in after_states]  # value of feature f for each of the after states
            chosen_after_state = after_states[np.argmax(feature_values)]  # after state with top feature value
            feature_validity = self.qq(tuple(chosen_after_state))  # max(self.qq(tuple(chosen_after_state)))
            feature_validities.append(feature_validity)
        self.validities[state] = feature_validities
        return feature_validities

    def ttb_action(self, feature_importance, after_states):
        """
        implements the ttb heuristic for action selection

        :param feature_importance: a ranking of the importance of features
        :param after_states: actions to choose from
        :return: the best action, according to TTB
        """

        original_actions = after_states
        after_states = np.unique(after_states, axis=0)
        feature_importance = np.array(feature_importance, dtype=float)
        for i in range(self.num_features):
            # TODO add in something for feature direction
            best_feature = np.nanargmax(feature_importance)
            feature_values = [a[best_feature] for a in after_states]
            if np.sum(feature_values == max(feature_values)) == 1:  # if there is 1 action with the max feature value
                action_ix = np.argmax(feature_values)  # select that action. Otherwise, keep looping through features
                break
            else:  # find the next best deciding feature - delete previous best. Only keep after_states which were best
                after_states = after_states[np.argwhere(feature_values == np.max(feature_values)).flatten()]
                feature_importance[best_feature] = np.nan
        else:
            action_ix = np.random.choice(after_states.shape[0])
        best_after_state = after_states[action_ix]
        whole_list_action_ix = int(np.where(np.all(original_actions == best_after_state, axis=1))[0][0])
        return whole_list_action_ix, best_after_state



class ExploitV:
    def __init__(self, num_features=8, *args):  # qq
        self.validities = defaultdict(lambda: np.zeros(4))
        self.num_features = num_features

    def qq(self, x):
        return 4 * x[2] + x[4] + x[5] + x[1] + x[3] + x[6]

    def choose_action(self, state, after_states):
        state = tuple(state)  # make state hashable
        if sum(self.validities[state]) == 0:
            validities = self.calc_validities(state, after_states)
        else:
            validities = self.validities[state]
        action_ix, after_state = self.ttb_action(validities, after_states)
        # print(action_ix, after_state)
        return action_ix

    def calc_validities(self, state, after_states):
        """
        Feature validity of feature i is calculated as max_a(Q(s,a)) for the after state s that would be chosen with
        feature i set as the best feature. There is no conditionality in this calculation
        :param state: current state
        :param after_states: states each of the actions would lead to
        :return: a list of feature validities, indexed in the same order as the state features
        """

        # In state s you have n afterstates, each afterstate with an associated value.
        # A state dependent TTB would find for each state the feature validities.
        # How? For each feature, find the value of the afterstate it would have selected
        # Feature validities correspond to these values
        feature_validities = []
        for f in range(self.num_features):
            feature_values = [a[f] for a in after_states]  # value of feature f for each of the after states
            chosen_after_state = after_states[np.argmax(feature_values)]  # after state with top feature value
            feature_validity = self.qq(tuple(chosen_after_state))  # value of chosen state (now validity of feature f
            feature_validities.append(feature_validity)  # save validity
        self.validities[state] = feature_validities
        # print(after_states, feature_validities)
        return feature_validities

    def ttb_action(self, feature_importance, after_states):
        """
        implements the ttb heuristic for action selection

        :param feature_importance: a ranking of the importance of features
        :param after_states: actions to choose from
        :return: the best action, according to TTB
        """

        original_actions = after_states
        after_states = np.unique(after_states, axis=0)
        feature_importance = np.array(feature_importance, dtype=float)
        for i in range(self.num_features):
            # TODO add in something for feature direction
            best_feature = np.nanargmax(feature_importance)
            feature_values = [a[best_feature] for a in after_states]
            if np.sum(feature_values == max(feature_values)) == 1:  # if there is 1 action with the max feature value
                action_ix = np.argmax(feature_values)  # select that action. Otherwise, keep looping through features
                break
            else:  # find the next best deciding feature - delete previous best. Only keep after_states which were best
                after_states = after_states[np.argwhere(feature_values == np.max(feature_values)).flatten()]
                feature_importance[best_feature] = np.nan
        else:
            action_ix = np.random.choice(after_states.shape[0])
        best_after_state = after_states[action_ix]
        whole_list_action_ix = int(np.where(np.all(original_actions == best_after_state, axis=1))[0][0])
        return whole_list_action_ix, best_after_state
